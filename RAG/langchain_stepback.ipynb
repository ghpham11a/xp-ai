{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bedb5c0-e743-4679-91f2-ca3471566525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain langchain-chroma python-dotenv bs4 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e58014d8-3ad9-49e6-9850-eac0f91525fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d28e59af-6420-4683-8691-b9f6dc7bd8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure these Enviornment varaibles are set\n",
    "\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"<>\"\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"<>\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"<>\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c74bc2a-ac7d-4ad9-bda3-f57dd4c09b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING: Reading Data ####\n",
    "\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc994a99-c5d8-42c5-a559-752451ce3a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING: Spliting Data ####\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "686f066c-8315-4314-bbf3-fdf19c0c81ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to persit Chroma and load it again later\n",
    "\n",
    "########################################\n",
    "# Save\n",
    "########################################\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    persist_directory=\"./chroma_db\"  # Specify the directory where you want to persist data\n",
    ")\n",
    "\n",
    "########################################\n",
    "# Load\n",
    "########################################\n",
    "\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "019a95ac-8a38-4d2d-b025-912461307872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "# We now transform these to example messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61a0aac4-7cda-424c-b8e0-c6fbd06f8f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the process of breaking down tasks for LLM agents?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "generate_queries_step_back = prompt | ChatOpenAI(temperature=0) | StrOutputParser()\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_queries_step_back.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54dc6d6a-8560-4a3c-b811-5e7459ce4bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents refers to the process of breaking down complex tasks into smaller and more manageable subtasks that can be easily handled by the agent. This approach allows LLM-powered autonomous agents to effectively tackle intricate tasks by dividing them into simpler steps, thereby enhancing their performance and efficiency.\\n\\nOne common technique used for task decomposition in LLM agents is the Chain of Thought (CoT) method, which involves prompting the model to \"think step by step\" and decompose difficult tasks into smaller components. This technique enables the agent to utilize more computational resources during testing to effectively break down complex tasks.\\n\\nAnother advanced method is the Tree of Thoughts, which extends the CoT approach by exploring multiple reasoning possibilities at each step. This method generates multiple thoughts per step, creating a tree structure that helps in analyzing various paths to solve a problem. The search process in the Tree of Thoughts can be conducted using BFS (breadth-first search) or DFS (depth-first search), with each state evaluated by a classifier or majority vote.\\n\\nTask decomposition for LLM agents can be achieved through various means, including simple prompting like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ\" within the LLM model. Additionally, task-specific instructions can be provided to guide the agent, such as \"Write a story outline\" for writing a novel. Human inputs can also be utilized for task decomposition, allowing for a more interactive and collaborative approach to problem-solving.\\n\\nOverall, task decomposition plays a crucial role in enhancing the capabilities of LLM-powered autonomous agents by breaking down complex tasks into manageable steps, enabling more efficient and effective problem-solving strategies.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Response prompt\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass on the question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
